{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea5f6ab7",
   "metadata": {},
   "source": [
    "# Improved Deepfake Detector V4 - Large Dataset Training (40k)\n",
    "\n",
    "**Based on successful model3.ipynb architecture (PROVEN TO WORK)**\n",
    "\n",
    "**Dataset:** 48k images (40k train, 4k test, 4k validate) - 4x larger than original V4\n",
    "\n",
    "**Previous Results with 20k train:**\n",
    "- Test Accuracy: 92%\n",
    "- Fake Detection: ~85-87%\n",
    "- Confusion matrix showed excellent performance\n",
    "\n",
    "**Target with 40k train:** >93% accuracy, >88% fake detection\n",
    "\n",
    "**Strategy:**\n",
    "1. **Binary crossentropy** (proven loss function)\n",
    "2. **Class weights 2.0x** for fakes (penalize missing fakes)\n",
    "\n",
    "3. **Extended epochs:** 30+40 (scaled for 4x data vs original)5. **Clean epoch-by-epoch output**\n",
    "4. **Proven architecture:** 1024‚Üí512‚Üí384‚Üí256‚Üí128 dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56761c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.applications import EfficientNetV2B0\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "IMG_SIZE = (256, 256)\n",
    "BATCH_SIZE = 16\n",
    "data_dir = \"/home/wizz/ML Project/Dataset\"\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "train_ds = image_dataset_from_directory(\n",
    "    os.path.join(data_dir, 'train'),\n",
    "    seed=42,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='binary'\n",
    ")\n",
    "\n",
    "val_ds = image_dataset_from_directory(\n",
    "    os.path.join(data_dir, 'validate'),\n",
    "    seed=42,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='binary'\n",
    ")\n",
    "\n",
    "test_ds = image_dataset_from_directory(\n",
    "    os.path.join(data_dir, 'test'),\n",
    "    seed=42,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Data augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.RandomContrast(0.1),\n",
    "    layers.RandomBrightness(0.1),\n",
    "], name='augmentation')\n",
    "\n",
    "def preprocess(images, labels):\n",
    "    images = data_augmentation(images, training=True)\n",
    "    return images, labels\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.map(preprocess, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(AUTOTUNE)\n",
    "test_ds = test_ds.prefetch(AUTOTUNE)\n",
    "\n",
    "train_batches = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "val_batches = tf.data.experimental.cardinality(val_ds).numpy()\n",
    "test_batches = tf.data.experimental.cardinality(test_ds).numpy()\n",
    "\n",
    "print(f\"\\n‚úÖ Datasets loaded:\")\n",
    "print(f\"   Train: {train_batches * BATCH_SIZE} images (40,000 expected)\")\n",
    "print(f\"   Val:   {val_batches * BATCH_SIZE} images (4,000 expected)\")\n",
    "print(f\"   Test:  {test_batches * BATCH_SIZE} images (4,000 expected)\")\n",
    "print(f\"\\nüìä Dataset is 4x larger than original - using extended epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e901e1",
   "metadata": {},
   "source": [
    "## Build Model - PROVEN ARCHITECTURE\n",
    "\n",
    "**Same successful architecture from model3 + V4:**\n",
    "- EfficientNetV2B0 backbone + 1024‚Üí512‚Üí384‚Üí256‚Üí128 dense layers\n",
    "- Dropout: 0.5‚Üí0.4‚Üí0.35‚Üí0.3‚Üí0.2 (prevents overfitting)\n",
    "- L2 regularization (0.001) on dense layers\n",
    "- Binary crossentropy loss (PROVEN, not focal loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba228f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model - based on model3 architecture\n",
    "inputs = Input(shape=IMG_SIZE + (3,))\n",
    "\n",
    "# Normalization\n",
    "x = layers.Rescaling(1./127.5, offset=-1)(inputs)  # [-1, 1] normalization\n",
    "\n",
    "# Use EfficientNetV2 as backbone\n",
    "backbone = EfficientNetV2B0(include_top=False, weights='imagenet', input_tensor=x)\n",
    "backbone.trainable = False  # Freeze initially\n",
    "\n",
    "backbone_output = backbone.output\n",
    "x = layers.GlobalAveragePooling2D()(backbone_output)\n",
    "\n",
    "# Dense layers - PROVEN ARCHITECTURE FROM MODEL3 + V4\n",
    "x = layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "x = layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "\n",
    "# Extra layer for improved feature discrimination\n",
    "x = layers.Dense(384, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.35)(x)\n",
    "\n",
    "x = layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "# Output layer\n",
    "outputs = layers.Dense(1, activation='sigmoid', name='output')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs, name='DeepfakeDetector_V4')\n",
    "\n",
    "# Compile with BINARY CROSSENTROPY (proven to work in model3 and V4)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(learning_rate=0.001, weight_decay=0.0001),\n",
    "    loss='binary_crossentropy',  # BACK TO PROVEN LOSS FUNCTION\n",
    "    metrics=['accuracy', \n",
    "             tf.keras.metrics.Precision(name='precision'),\n",
    "             tf.keras.metrics.Recall(name='recall'),\n",
    "             tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model built - Params: {model.count_params():,}\")\n",
    "print(f\"üìå Using binary_crossentropy (proven) with class_weight=2.0x for fakes\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc62b51",
   "metadata": {},
   "source": [
    "## Phase 1: Train Classifier - 40k Training Images\n",
    "\n",
    "**Proven strategy scaled for 4x larger dataset:**\n",
    "- Binary crossentropy loss\n",
    "- Class weights: Real=1.0, Fake=2.0\n",
    "- Monitor: val_loss (decreasing = improving)\n",
    "- 30 epochs (scaled for 40k images), patience=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbe55ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class weights - proven approach from model V4\n",
    "class_weight = {\n",
    "    0: 1.0,   # Real images\n",
    "    1: 2.0    # Fake images - 2x penalty\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PHASE 1: Training classifier (frozen backbone)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Loss: binary_crossentropy | Class weights: {class_weight}\")\n",
    "print(f\"Training on 40,000 images (20k real + 20k fake)\")\n",
    "print(f\"Epochs: 30 | Monitoring: val_loss\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Callbacks - SIMPLE AND PROVEN\n",
    "callbacks_phase1 = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',  # Monitor LOSS, not AUC\n",
    "        patience=8,\n",
    "        restore_best_weights=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',  # Monitor LOSS, not AUC\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'best_recall_phase1.h5',\n",
    "        monitor='val_recall',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "history1 = model.fit(\n",
    "    train_ds,\n",
    "    epochs=30,  # Increased for 40k training images\n",
    "    epochs=25,\n",
    "    class_weight=class_weight,\n",
    "    callbacks=callbacks_phase1,\n",
    "    verbose=1  # Clean epoch-by-epoch output\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PHASE 1 COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Best Accuracy: {max(history1.history['val_accuracy']):.2%}\")\n",
    "print(f\"Best Recall:   {max(history1.history['val_recall']):.2%}\")\n",
    "print(f\"Best AUC:      {max(history1.history['val_auc']):.4f}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cb1dfe",
   "metadata": {},
   "source": [
    "## Phase 2: Fine-tune - 40k Training Images\n",
    "\n",
    "Unfreeze backbone and fine-tune with 40k training images.\n",
    "40 epochs (scaled for larger dataset), patience=10, monitor val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c47a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PHASE 2: Fine-tuning entire model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Unfreeze backbone\n",
    "backbone.trainable = True\n",
    "print(f\"Trainable params: {model.count_params():,}\")\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-5, weight_decay=0.0001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy',\n",
    "             tf.keras.metrics.Precision(name='precision'),\n",
    "             tf.keras.metrics.Recall(name='recall'),\n",
    "             tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "print(f\"Training on 40,000 images\")\n",
    "print(f\"Epochs: 40 | Monitoring: val_loss\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Callbacks - SIMPLE AND PROVEN\n",
    "callbacks_phase2 = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',  # Monitor LOSS, not AUC\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',  # Monitor LOSS, not AUC\n",
    "        factor=0.3,\n",
    "        patience=4,\n",
    "        min_lr=1e-8,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'best_recall_phase2.h5',\n",
    "        monitor='val_recall',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "history2 = model.fit(\n",
    "    train_ds,\n",
    "    epochs=40,  # Increased for 40k training images\n",
    "    epochs=35,\n",
    "    class_weight=class_weight,\n",
    "    callbacks=callbacks_phase2,\n",
    "    verbose=1  # Clean epoch-by-epoch output\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PHASE 2 COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Final Accuracy: {max(history2.history['val_accuracy']):.2%}\")\n",
    "print(f\"Final Recall:   {max(history2.history['val_recall']):.2%}\")\n",
    "print(f\"Final AUC:      {max(history2.history['val_auc']):.4f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "model.save('deepfake_detector_v4_40k.h5')\n",
    "print(\"‚úÖ Model saved: deepfake_detector_v4_40k.h5\")\n",
    "\n",
    "print(\"üìä Trained on 40,000 images (20k real + 20k fake)\")print(\"‚úÖ Model saved: deepfake_detector_v4_large_dataset.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dfd993",
   "metadata": {},
   "source": [
    "## Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0821d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "metrics = ['loss', 'accuracy', 'precision', 'recall', 'auc']\n",
    "titles = ['Loss', 'Accuracy', 'Precision', 'Recall (Fake Detection)', 'AUC']\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    # Phase 1\n",
    "    phase1_train = history1.history[metric]\n",
    "    phase1_val = history1.history[f'val_{metric}']\n",
    "    epochs1 = range(1, len(phase1_train) + 1)\n",
    "    \n",
    "    # Phase 2\n",
    "    phase2_train = history2.history[metric]\n",
    "    phase2_val = history2.history[f'val_{metric}']\n",
    "    epochs2 = range(len(phase1_train) + 1, len(phase1_train) + len(phase2_train) + 1)\n",
    "    \n",
    "    # Plot\n",
    "    ax.plot(epochs1, phase1_train, 'b-', label='Phase 1 Train', linewidth=2)\n",
    "    ax.plot(epochs1, phase1_val, 'b--', label='Phase 1 Val', linewidth=2)\n",
    "    ax.plot(epochs2, phase2_train, 'r-', label='Phase 2 Train', linewidth=2)\n",
    "    ax.plot(epochs2, phase2_val, 'r--', label='Phase 2 Val', linewidth=2)\n",
    "    \n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Epoch', fontsize=11)\n",
    "    ax.set_ylabel(title, fontsize=11)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "fig.delaxes(axes[1, 2])\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history_v4_large.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VALIDATION METRICS - 40K TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Accuracy:  {history2.history['val_accuracy'][-1]:.2%} (Target: >93%)\")\n",
    "print(f\"Precision: {history2.history['val_precision'][-1]:.2%}\")\n",
    "print(f\"Recall:    {history2.history['val_recall'][-1]:.2%} (Target: >88%)\")\n",
    "print(f\"AUC:       {history2.history['val_auc'][-1]:.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüí° Previous 20k train: 92% accuracy, ~85-87% recall\")print(\"   Goal with 40k train: >93% accuracy, >88% recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c60f07",
   "metadata": {},
   "source": [
    "## Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30c4155",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_results = model.evaluate(test_ds, verbose=1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL V4 TEST RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test Loss:      {test_results[0]:.4f}\")\n",
    "print(f\"Test Accuracy:  {test_results[1]:.4f} ({test_results[1]*100:.2f}%)\")\n",
    "print(f\"Test Precision: {test_results[2]:.4f}\")\n",
    "print(f\"Test Recall:    {test_results[3]:.4f}\")\n",
    "print(f\"Test AUC:       {test_results[4]:.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get predictions for detailed analysis\n",
    "print(\"\\nGenerating predictions for detailed analysis...\")\n",
    "y_true = []\n",
    "y_pred_probs = []\n",
    "\n",
    "for images, labels in test_ds:\n",
    "    predictions = model.predict(images, verbose=0)\n",
    "    y_pred_probs.extend(predictions.flatten())\n",
    "    y_true.extend(labels.numpy())\n",
    "\n",
    "y_true = np.array(y_true, dtype=np.float32)\n",
    "y_pred_probs = np.array(y_pred_probs, dtype=np.float32)\n",
    "y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "# Calculate false negatives\n",
    "false_negatives = np.sum((y_true == 1) & (y_pred == 0))\n",
    "total_fakes = np.sum(y_true == 1)\n",
    "fn_rate = false_negatives / total_fakes\n",
    "\n",
    "# Calculate false positives  \n",
    "false_positives = np.sum((y_true == 0) & (y_pred == 1))\n",
    "total_reals = np.sum(y_true == 0)\n",
    "fp_rate = false_positives / total_reals\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON - PROGRESSIVE IMPROVEMENT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Metric':<25} | {'Model 3':<12} | {'V4 (12k)':<12} | {'V4 (20k)':<12} | {'V4 (40k)':<12} | {'Change'}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Overall Accuracy':<25} | {'87.25%':<12} | {'89.15%':<12} | {'~92.00%':<12} | {f'{test_results[1]*100:.2f}%':<12} | {(test_results[1]-0.92)*100:+.2f}%\")\n",
    "print(f\"{'Fake Detection (Recall)':<25} | {'77.50%':<12} | {'81.10%':<12} | {'~87.00%':<12} | {f'{test_results[3]*100:.2f}%':<12} | {(test_results[3]-0.87)*100:+.2f}%\")\n",
    "print(f\"{'False Negatives':<25} | {'225':<12} | {'189':<12} | {'~130':<12} | {f'{false_negatives}':<12} | {false_negatives-130:+.0f}\")\n",
    "print(f\"{'False Negative Rate':<25} | {'22.50%':<12} | {'18.90%':<12} | {'~13.00%':<12} | {f'{fn_rate*100:.2f}%':<12} | {(fn_rate-0.13)*100:+.2f}%\")\n",
    "print(f\"{'Real Detection':<25} | {'97.00%':<12} | {'97.20%':<12} | {'~97.50%':<12} | {f'{(1-fp_rate)*100:.2f}%':<12} | {((1-fp_rate)-0.975)*100:+.2f}%\")\n",
    "print(f\"{'Precision':<25} | {'96.27%':<12} | {'96.87%':<12} | {'~97.00%':<12} | {f'{test_results[2]*100:.2f}%':<12} | {(test_results[2]-0.97)*100:+.2f}%\")\n",
    "print(f\"{'AUC':<25} | {'95.93%':<12} | {'96.34%':<12} | {'~96.80%':<12} | {f'{test_results[4]*100:.2f}%':<12} | {(test_results[4]-0.968)*100:+.2f}%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Success criteria for 40k training\n",
    "target_recall = 0.88  # 88% fake detection = 12% FN rate\n",
    "target_accuracy = 0.93  # 93% overall accuracy\n",
    "target_fn = 120  # Out of 1000 fakes (improved from ~130 with 20k)\n",
    "\n",
    "if test_results[3] >= target_recall and test_results[1] >= target_accuracy:\n",
    "    print(\"\\nüéØ EXCELLENT: Target achieved with 40k training!\")\n",
    "    print(f\"   Accuracy: {test_results[1]*100:.2f}% (Target: ‚â•93%)\")\n",
    "    print(f\"   Fake detection: {test_results[3]*100:.2f}% (Target: ‚â•88%)\")\n",
    "    print(f\"   False negatives: {false_negatives} (Target: <{target_fn})\")\n",
    "    print(f\"   Improvement from 20k: {int((0.87-test_results[3])*-1000)} fewer missed fakes\")\n",
    "elif test_results[3] >= 0.85 or test_results[1] >= 0.92:  # Close to target\n",
    "    print(\"\\n‚úÖ STRONG IMPROVEMENT: Close to target!\")\n",
    "    print(f\"   Accuracy: {test_results[1]*100:.2f}% (Target: 93%)\")\n",
    "    print(f\"   Fake detection: {test_results[3]*100:.2f}% (Target: 88%)\")\n",
    "    print(f\"   False negatives: {false_negatives}\")\n",
    "    print(\"\\nüí° Fine-tune: Try threshold=0.45-0.48 to optimize balance\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Below expected improvement\")\n",
    "print(f\"   1. Update predict_gui.py to use: deepfake_detector_v4_40k.h5\")\n",
    "print(f\"   2. If recall <88%: Try threshold=0.45-0.48 for better fake detection\")\n",
    "print(f\"   3. Test best_recall_phase2.h5 if you need maximum fake detection\")\n",
    "\n",
    "print(f\"   4. Deploy model - 40k training should give production-ready results\")\n",
    "    print(f\"   3. Evaluate best_recall_phase2.h5 if recall is highest priority\")\n",
    "\n",
    "print(\"\\nüí° Next steps:\")print(f\"   2. Test threshold adjustment (0.45-0.48) for optimal balance\")\n",
    "print(f\"   1. Update predict_gui.py to use: deepfake_detector_v4_large_dataset.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
